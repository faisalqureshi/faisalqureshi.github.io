<!DOCTYPE html>
<html lang="en">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-G9CWRQBRB0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-G9CWRQBRB0');
    </script>

    <meta charset="utf-8">
    <meta name="generator" content="pandoc">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>A Residual-Dyad Encoder Discriminator Network for Remote Sensing Image Matching</title>
    <meta name="author" content="">
    <meta name="author" content="">
    <meta name="author" content="">
    <meta name="author" content="">

    <!-- css -->
    <link rel="stylesheet" href="../../style/bootstrap-3.3.7-dist/css/bootstrap.min.css" type="text/css" />
    <link rel="stylesheet" href="../../style/my.css" type="text/css" />
    <link rel="stylesheet" href="../../style/projects.css" type="text/css" />

    <!-- Google Fonts -->
    <link href='https://fonts.googleapis.com/css?family=BioRhyme:200,300,400,700,800' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Raleway:400,300,700' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=VT323:400,300,700' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Baloo:400,300,700' rel='stylesheet' type='text/css'>
    <link href="https://fonts.googleapis.com/css?family=Averia+Serif+Libre" rel="stylesheet">

    <!-- Scripts -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.0/jquery.min.js"></script>
    <script src="/style/my.js"></script>
  </head>
  <body style="background-color: white;">
    <div class="container" style="background: white;">

      <h1 class="project-title">A Residual-Dyad Encoder Discriminator Network for Remote Sensing Image Matching</h1>
      

      <hr>
      <div class="row">
        <div class="project-member">Numan Khurshid<sup>1</sup></div>
      </div>
      <div class="row">
        <div class="project-member">Mohbat<sup>1</sup></div>
      </div>
      <div class="row">
        <div class="project-member">Murtaza Taj<sup>1</sup></div>
      </div>
      <div class="row">
        <div class="project-member"><a href="http://faculty.uoit.ca/qureshi">Faisal Z. Qureshi</a><sup>2</sup></div>
      </div>
      <div class="row">
        <div class="project-lab-address">
          <div class="project-institute">
          Computer Vision and Graphics Lab<sup>1</sup><br>Department of Computer Science<br>Lahore School of Management Sciences<br>Lahore, Pakistan</div>
          <div class="project-institute">
          <a href="http://vclab.science.uoit.ca">Visual Computing Lab</a><sup>2</sup><br>Faculty of Science<br>University of Ontario Institute of Technology<br>2000 Simcoe St. N., Oshawa ON L1G 0C5</div>
        </div>
      </div>
      <hr>

      <h1 id="abstract">Abstract</h1>
<p>We propose a new method for remote sensing image matching. The
proposed method uses encoder subnetwork of an autoencoder pre-trained on
GTCrossView data to construct image features. A discriminator network
trained on University of California Merced Land Use/Land Cover dataset
(LandUse) and High-resolution Satellite Scene dataset (SatScene)
computes a match score between a pair of computed image features. We
also propose a new network unit, called residual-dyad, and empirically
demonstrate that networks that use residual-dyad units outperform those
that do not. We compare our approach with both traditional and more
recent learning-based schemes on LandUse and SatScene datasets, and the
proposed method achieves state-of-the-art result in terms of mean
average precision and ANMRR metrics. Specifically, our method achieves
an overall improvement in performance of 11.26% and 22.41%,
respectively, for LandUse and SatScene benchmark datasets.</p>
<hr>
<div class="row">
<div class="col-lg-12">
<figure class="figure" style="margin-top: 15px; margin-bottom: 20px;">
<center>
<a href="fig4.png"><img class="img-responsive" width="90%" src="fig4.png" alt="Proposed residual dyad unit."></a>
<figcaption class="figure-caption" style="font-style: italic; font-weight: bold;">
Proposed residual dyad unit.
</figcaption>
</center>
</figure>
</div>
</div>
<div class="row">
<div class="col-lg-12">
<figure class="figure" style="margin-top: 15px; margin-bottom: 20px;">
<center>
<a href="fig5.png"><img class="img-responsive" width="90%" src="fig5.png" alt="Unsupervised Autoencoder Features: Image input from left (to encoder sub-network) and outputs to the right of (decoder) network. z is taken as the feature vector of the given image."></a>
<figcaption class="figure-caption" style="font-style: italic; font-weight: bold;">
Unsupervised Autoencoder Features: Image input from left (to encoder
sub-network) and outputs to the right of (decoder) network. z is taken
as the feature vector of the given image.
</figcaption>
</center>
</figure>
</div>
</div>
<div class="row">
<div class="col-lg-12">
<figure class="figure" style="margin-top: 15px; margin-bottom: 20px;">
<center>
<a href="fig6.png"><img class="img-responsive" width="90%" src="fig6.png" alt="Network architecture of the proposed ResDyadDML that takes features from ResDyadAE for an image pair and predicts the matching score. Residual-dyad block has been integrated to boost the performance of the network."></a>
<figcaption class="figure-caption" style="font-style: italic; font-weight: bold;">
Network architecture of the proposed ResDyadDML that takes features from
ResDyadAE for an image pair and predicts the matching score.
Residual-dyad block has been integrated to boost the performance of the
network.
</figcaption>
</center>
</figure>
</div>
</div>
<div class="row">
<div class="col-lg-12">
<figure class="figure" style="margin-top: 15px; margin-bottom: 20px;">
<center>
<a href="fig1.png"><img class="img-responsive" width="90%" src="fig1.png" alt="Finding matching images in LandUse dataset."></a>
<figcaption class="figure-caption" style="font-style: italic; font-weight: bold;">
Finding matching images in LandUse dataset.
</figcaption>
</center>
</figure>
</div>
</div>
<div class="row">
<div class="col-lg-12">
<figure class="figure" style="margin-top: 15px; margin-bottom: 20px;">
<center>
<a href="fig2.png"><img class="img-responsive" width="90%" src="fig2.png" alt="Comparative results of class-wise mAP among supervised (VGG16, GoogleNet, and SatResNet-50) and our proposed approach."></a>
<figcaption class="figure-caption" style="font-style: italic; font-weight: bold;">
Comparative results of class-wise mAP among supervised (VGG16,
GoogleNet, and SatResNet-50) and our proposed approach.
</figcaption>
</center>
</figure>
</div>
</div>
<div class="row">
<div class="col-lg-12">
<figure class="figure" style="margin-top: 15px; margin-bottom: 20px;">
<center>
<a href="fig3.png"><img class="img-responsive" width="90%" src="fig3.png" alt="The proposed model seems to construct deep features that may be useful in domains other than remote image sensing."></a>
<figcaption class="figure-caption" style="font-style: italic; font-weight: bold;">
The proposed model seems to construct deep features that may be useful
in domains other than remote image sensing.
</figcaption>
</center>
</figure>
</div>
</div>
<h1 id="publication">Publication</h1>
<p>For technical details please look at the following publications</p>
<script src="https://bibbase.org/show?bib=http://vclab.science.uoit.ca/faisal-qureshi.bib&jsonp=1&filter=keywords:residual-dyad&group0=type"></script>


      <footer style="margin-top: 50px;">
  <div class="container footer">
    <div class="row">
      <hr>
      <div class="col-md-6 col-sm-12">
        <img src="http://vclab.science.uoit.ca/imgs/vclab-ontariotech.png">
      </div>
      <div class="col-md-6 col-sm-12">
        &copy; Faisal Qureshi<br>
        Last updated <br>
        Site generated using &copy; Webify (Ver. 4.1)
      </div>
</footer>
      
    </div>

  </body>
</html>
