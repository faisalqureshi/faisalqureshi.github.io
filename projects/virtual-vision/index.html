<html>
	<link rel="stylesheet" type="text/css" href="../../site.css" />
	<link rel="stylesheet" type="text/css" href="../../project.css" />
	
    <link href="http://fonts.googleapis.com/css?family=Ubuntu:bold" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=Vollkorn" rel="stylesheet" type="text/css">
   <link href="http://fonts.googleapis.com/css?family=Molengo"
    rel="stylesheet" type="text/css">
	


    <title>Virtual Vision for Smart Camera Sensor Networks</title>
    
    <head>
	<meta name="keywords" content="multi-camera systems, camera networks, smart cameras, sensor networks, intelligent perception, computer vision">
<script type="text/javascript">

 var _gaq = _gaq || [];
 _gaq.push(['_setAccount', 'UA-28175853-1']);
 _gaq.push(['_trackPageview']);

 (function() {
   var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
   ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
   var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
 })();

</script>
	</head>

<body class="oneColElsCtr">

<div id="container">
  <div id="mainContent">

	<div class="title">
		<h1>Virtual Vision <br>
	    for Smart Camera Sensor Networks</h1>
<h2>        <a href="http://faculty.uoit.ca/qureshi/">Faisal Z. Qureshi</a> and <a href="http://cs.ucla.edu/~dt">Demetri Terzopoulos</a></h2>
       </div> 
                <h2>Synopsis</h2>
    <p>Virtual worlds can serve as software laboratories for carrying out camera networks research. Espousing this unorthodox view, we developed virtual vision paradigm that brings together advanced computer graphics and vision simulation technologies to serve the needs of camera networks research. Visually and behaviorally realistic environments, populated with self-animating autonomous pedestrians are used to study, develop, and evaluate vision systems [<a href="#1">1</a>]. Shao and Terzopoulos [<a href="#2">2</a>] developed such a 3D environment---a 3D reconstruction of the original Penn train station (Fig. 1). Within this environment virtual cameras generate synthetic video feeds. The video streams emulate those generated by real surveillance cameras, and low-level image processing mimics the performance characteristics of a state-of-the-art surveillance video system.
    A consequence of this paradigm is our recent work on developing smart camera networks and intelligent surveillance systems capable of persistent human observation in large-scale synthetic environments.</p>

        
	<p align="center"><a href="vv2.png"><img src="vv2-small.png" width="800" height="898" alt="Virtual Vision Paradigm"></a></p>
    <h2>Virtual Vision vs. Real World</h2>
  <p align="center"><img src="vv-rv-web.png" alt="" height="254" width="500" border="1"></p>
	  
	<h2>Publications</h2>
    
    

    <p>&quot;<a href="../../virtual-vision/pubs/08-pieee-j.pdf"  class="papertitle">Smart Camera Networks in Virtual Reality</a>,&quot; <span class="paperauthfzq">F.Z. Qureshi</span>, D. Terzopoulos, <span class="papervenue">Proceedings of the IEEE</span>, <strong>96</strong>(10), October, 2008, 1640&mdash;1656, (Special Issue on &quot;Smart Cameras&quot;).    </p>
    <p>&quot;<a href="../../virtual-vision/pubs/07-vrst-c.pdf" class="papertitle">Virtual Vision: Visual Sensor Networks in Virtual Reality</a>,'' <span class="paperauthfzq">F.Z. Qureshi</span>, D. Terzopoulos, <span class="papervenue">Proc. ACM Symposium on Virtual Reality Software and Technology (VRST 2007)</span>, Newport Beach, CA, November, 2007, 247&mdash;248.</p>
      <h2>Movies</h2>
      <p>The following movies showcase virtual vision paradigm for camera networks research. </p>
<h3 align="left">Camera Tasking through announcement, bidding, and selection</h3>
    
    <p align="center"><iframe width="420" height="345" src="http://www.youtube.com/embed/Xjs14ZhBn5g" frameborder="0" allowfullscreen><br></iframe></p>
    
    <h3 align="left">Persistent Coverage</h3>
    
    <p align="left">Cameras decide among themselves how best to observe the pedestrian as she makes her way through the virtual Penn station. An operator selects the pedestrian to be tracked in camera 7.</p>
        
    <p align="center"><iframe width="420" height="345" src="http://www.youtube.com/embed/b067HAGQgOQ" frameborder="0" allowfullscreen></iframe></p>
        
    <h3 align="left">Tracking</h3>
        
    <p align="left">We developed appearance-based pedestrian tracker that mimic the performance of trackers deployed on physical systems.</p>
        
        <p align="center"><iframe width="420" height="345" src="http://www.youtube.com/embed/1OoyEKLr0jY" frameborder="0" allowfullscreen></iframe></p>
        
        <p align="center"><iframe width="420" height="345" src="http://www.youtube.com/embed/8A2DMR3PgT0?hl=en&fs=1" frameborder="0" allowfullscreen></iframe></p>
    
<h3 align="left">Fixation and Zooming</h3>
                
<p align="left">Image drive fixation and zooming routines.</p>
		
        <p align="center"><iframe width="425" height="349" src="http://www.youtube.com/embed/3zcvfZKueic?hl=en&fs=1" frameborder="0" allowfullscreen></iframe></p>
        
        <p align="center"><iframe width="420" height="345" src="http://www.youtube.com/embed/ngXsi7vFAv0" frameborder="0" allowfullscreen></iframe></p>        
        
<p align="left">PTZ Camera Stabilization while zooming and fixating</p>
  
        <p align="center"><iframe width="425" height="349" src="http://www.youtube.com/embed/LQBtN8_ch2s?hl=en&fs=1" frameborder="0" allowfullscreen></iframe></p>

<h3 align="left">Background Subtraction</h3>
        
        <p align="left">Background subtraction for passive wide-FOV cameras to identify foreground objects. We do not rely upon background subtraction for PTZ cameras. Background model is learnt over time through observation. The model is also periodically updated to account for changes in the background. Note that background subtraction is not perfect (as when using real video).</p>
        
        <p align="center"><iframe width="420" height="345" src="http://www.youtube.com/embed/XBK1W_3nNgE" frameborder="0" allowfullscreen></iframe>
        <iframe width="420" height="345" src="http://www.youtube.com/embed/gbCu6zpane0" frameborder="0" allowfullscreen></iframe></p>
        
<h2 align="left"><a id="acknowledgements" name="acknowledgements"></a>Acknowledgements</h2>
		
        <p align="left">The research reported herein was made possible in part by a grant from the Defence Advanced Research Projects Agency (DARPA) of the Department of Defence. We thank Dr. Tom Strat, formerly of DARPA, for his generous support and encouragement. We also thank Wei Shao and Mauricio Plaza-Villegas for their invaluable contributions to the implementation of the Penn Station simulator.</p>
		<h2 align="left"><a id="references" name="references"></a>References</h2>
		<p align="left"><a id="1" name="1"></a>[1] <a href="http://www.cs.ucla.edu/%7edt/">D. Terzopoulos</a>, &ldquo;Perceptive agents and systems in virtual reality,&rdquo; in Proc. 10th ACM Symposium on Virtual Reality Software and Technology, (Osaka, Japan), pp.1&#150;3, Oct. 2003.</p>
		<p align="left"><a id="2" name="2"></a>[2] <a href="http://mrl.nyu.edu/%7eweishao/">W. Shao</a> and D.Terzopoulos, &ldquo;Autonomous pedestrians,&rdquo; in Proc. ACMSIGGRAPH/Eurographics Symposium on Computer Animation, (LosAngeles, CA), pp. 19&#150;28, July 2005.</p>
	</div>
    </div>
    </body>

</html>