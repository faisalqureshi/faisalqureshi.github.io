<html>
	<link rel="stylesheet" type="text/css" href="../../site.css" />
	<link rel="stylesheet" type="text/css" href="../../project.css" />
	<title>Virtual Vision for Smart Camera Sensor Networks</title>
	<meta name="keywords" content="multi-camera systems, camera networks, smart cameras, sensor networks, intelligent perception, computer vision">
	

	<body>
		<h1>Virtual Vision for Smart Camera Sensor Networks</h1>
	
        <h2 align="left"><a id="background" name="background"></a>Background</h2>
		
        <p align="left">Terzopoulos [<a href="#1">1</a>] proposed a Virtual Vision approach to designing surveillance systems using a virtual train station environment populated by fully autonomous, life like pedestrians that perform various activities [<a href="#2">2</a>]. Within this environment, virtual cameras generate synthetic video feeds. The video streams emulate those generated by real surveillance cameras, and low-level image processing mimics the performance characteristics of a state-of-the-art surveillance video system.</p>
        
		<p align="left">It is an on going project and the related publications are available <a href="http://www.cs.toronto.edu/%7efaisal">here</a>. </p>
		
        <h2 align="left"><a id="vv" name="vv"></a>Virtual Vision</h2>
		  <table width="122" border="1" cellspacing="2" cellpadding="0">
		    <tr>
		      <td><img src="vv.gif" alt="" height="240" width="322" border="0"></td>
		      <td><img src="vv1.gif" alt="" height="240" width="322" border="0"></td>
		      </tr>
		    </table>
	  <div align="left"><img src="vv-rv-web.png" alt="" height="254" width="500" border="1"></div>
	  
      <h2 align="left">Movies</h2>
      
      <h3 align="left">Camera Tasking through announcement, bidding, and selection</h3>
    
    <p align="left"><iframe width="420" height="345" src="http://www.youtube.com/embed/Xjs14ZhBn5g" frameborder="0" allowfullscreen><br></iframe></p>
    
    <h3 align="left">Persistent Coverage</h3>
    
    <p align="left">Cameras decide among themselves how best to observe the pedestrian as she makes her way through the virtual Penn station. An operator selects the pedestrian to be tracked in camera 7.</p>
        
    <p align="left"><iframe width="420" height="345" src="http://www.youtube.com/embed/b067HAGQgOQ" frameborder="0" allowfullscreen></iframe></p>
        
    <h3 align="left">Tracking</h3>
        
    <p align="left">We developed appearance-based pedestrian tracker that mimic the performance of trackers deployed on physical systems.</p>
        
        <p align="left"><iframe width="420" height="345" src="http://www.youtube.com/embed/1OoyEKLr0jY" frameborder="0" allowfullscreen></iframe>
        <iframe align="left" width="420" height="345" src="http://www.youtube.com/embed/8A2DMR3PgT0?hl=en&fs=1" frameborder="0" allowfullscreen></iframe></p>
    
<h3 align="left">Fixation and Zooming</h3>
                <p align="left">Image drive fixation and zooming routines.</p>
		
        <p align="left"><iframe width="425" height="349" src="http://www.youtube.com/embed/3zcvfZKueic?hl=en&fs=1" frameborder="0" allowfullscreen></iframe>
        <iframe width="420" height="345" src="http://www.youtube.com/embed/ngXsi7vFAv0" frameborder="0" allowfullscreen></iframe>        
        
        <p align="left">PTZ Camera Stabilization while zooming and fixating</p>
  
        <p align="left"><iframe width="425" height="349" src="http://www.youtube.com/embed/LQBtN8_ch2s?hl=en&fs=1" frameborder="0" allowfullscreen></iframe></p>

<h3 align="left">Background Subtraction</h3>
        
        <p align="left">Background subtraction for passive wide-FOV cameras to identify foreground objects. We do not rely upon background subtraction for PTZ cameras. Background model is learnt over time through observation. The model is also periodically updated to account for changes in the background. Note that background subtraction is not perfect (as when using real video).</p>
        
        <p align="left"><iframe width="420" height="345" src="http://www.youtube.com/embed/XBK1W_3nNgE" frameborder="0" allowfullscreen></iframe>
        <iframe width="420" height="345" src="http://www.youtube.com/embed/gbCu6zpane0" frameborder="0" allowfullscreen></iframe></p>
        
<h2 align="left"><a id="acknowledgements" name="acknowledgements"></a>Acknowledgements</h2>
		
        <p align="left">The research reported herein was made possible in part by a grant from the Defence Advanced Research Projects Agency (DARPA) of the Department of Defence. We thank Dr. Tom Strat, formerly of DARPA, for his generous support and encouragement. We also thank Wei Shao and Mauricio Plaza-Villegas for their invaluable contributions to the implementation of the Penn Station simulator.</p>
		<h2 align="left"><a id="references" name="references"></a>References</h2>
		<p align="left"><a id="1" name="1"></a>[1] <a href="http://www.cs.ucla.edu/%7edt/">D. Terzopoulos</a>, &ldquo;Perceptive agents and systems in virtual reality,&rdquo; in Proc. 10th ACM Symposium on Virtual Reality Software and Technology, (Osaka, Japan), pp.1&#150;3, Oct. 2003.</p>
		<p align="left"><a id="2" name="2"></a>[2] <a href="http://mrl.nyu.edu/%7eweishao/">W. Shao</a> and D.Terzopoulos, &ldquo;Autonomous pedestrians,&rdquo; in Proc. ACMSIGGRAPH/Eurographics Symposium on Computer Animation, (LosAngeles, CA), pp. 19&#150;28, July 2005.</p>
	</body>

</html>